{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_sequences(file_path, sequence_length=30):\n",
    "    data = np.load(file_path, allow_pickle=True).astype(np.float16)\n",
    "    sequences = []\n",
    "    for i in range(len(data) - sequence_length ):\n",
    "        sequence = data[i:i + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    sequences = np.array(sequences)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# a_sequences = prepare_sequences(r'C:\\Users\\oem\\Desktop\\jhy\\dataset2\\(It is impossible)\\raw_1707385020.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "1645 are ready\n",
      "13 개 그룹이 만들어질 예정입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [18:21, 84.74s/it]\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "# print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob  # glob 라이브러리 추가\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "os.makedirs(f'C:/Users/oem/Desktop/jhy/array/{start_time}',exist_ok=True)\n",
    "\n",
    "# dataset 폴더 경로 설정\n",
    "# PATH = '/content/drive/MyDrive/LAB/Sign_Language_Remaster/code/lstm/dataset'\n",
    "PATH = r'C:/Users/oem/Desktop/jhy/dataset'\n",
    "\n",
    "# dataset 폴더 아래의 모든 폴더 목록을 얻기\n",
    "actions = []  # 변경된 부분\n",
    "label_mapping = {}  # 변경된 부분\n",
    "\n",
    "### 액션 리스트 확인\n",
    "for folder in os.listdir(PATH):\n",
    "    if os.path.isdir(os.path.join(PATH, folder)):\n",
    "        # 각 폴더에 있는 npy 파일들을 확인\n",
    "        npy_files = glob.glob(os.path.join(PATH, folder, 'raw_*.npy'))\n",
    "        # 특정 조건에 맞는 npy 파일이 존재하면 actions와 label_mapping에 추가\n",
    "        if npy_files:\n",
    "            actions.append(folder)\n",
    "            label_mapping[folder] = len(actions) - 1\n",
    "print(len(actions),'are ready')\n",
    "\n",
    "# 리스트를 파일로 저장\n",
    "with open(r'C:/Users/oem/Desktop/jhy/signlanguage/Sign_Language_Remaster/logs/act_list.pkl', 'wb') as f:\n",
    "    pickle.dump(actions, f)\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "def create_groups(data_count, group_size, appearances_per_data):\n",
    "    # 데이터셋 초기화\n",
    "    datasets = list(range(0, data_count ))\n",
    "    random.shuffle(datasets)\n",
    "\n",
    "    # 각 데이터의 사용 횟수를 추적하는 딕셔너리\n",
    "    usage_count = {dataset: 0 for dataset in datasets}\n",
    "   \n",
    "    # 그룹을 저장할 리스트\n",
    "    groups = []\n",
    "   \n",
    "    # 현재 그룹\n",
    "    current_group = []\n",
    "   \n",
    "    # 데이터셋을 순환하면서 그룹 생성\n",
    "    for _ in range(appearances_per_data):\n",
    "        for dataset in datasets:\n",
    "            # 현재 그룹에 데이터 추가\n",
    "            current_group.append(dataset)\n",
    "            usage_count[dataset] += 1\n",
    "           \n",
    "            # 현재 그룹이 가득 찼거나 모든 데이터가 사용된 경우 그룹 저장 및 초기화\n",
    "            if len(current_group) == group_size or all(usage_count[dataset] == appearances_per_data for dataset in datasets):\n",
    "                groups.append(current_group)\n",
    "                current_group = []\n",
    "   \n",
    "    return groups\n",
    "\n",
    "# 4000개의 데이터셋으로 크기가 500인 그룹을 만들고, 각 데이터가 3번씩 포함되도록 그룹 생성\n",
    "groups = create_groups(len(actions), 400, 3)\n",
    "print(len(groups),'개 그룹이 만들어질 예정입니다.')\n",
    "for group_idx, group in tqdm(enumerate(groups)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    # print(len(group))\n",
    "    for act_idx in group:\n",
    "        # print(act_idx)\n",
    "        action = actions[act_idx]\n",
    "    \n",
    "################################################################################################################################\n",
    "        PATH2 = os.path.join(PATH, action) #/dataset/{action}\n",
    "        file_path = glob.glob(os.path.join(PATH2, f'raw_*.npy'))[0]\n",
    "        try:\n",
    "            loaded_data = prepare_sequences(file_path)  # raw 데이터를 seq화 함\n",
    "            # loaded_data = np.load(file_path, allow_pickle=True).astype(np.float16)\n",
    "            data.append(loaded_data)\n",
    "################################################################################################################################\n",
    "\n",
    "            label = np.full((loaded_data.shape[0],), label_mapping[action])\n",
    "            labels.append(label)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for action: {action}\")\n",
    "\n",
    "    x_data = np.concatenate(data, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    y_data = to_categorical(labels, num_classes=len(actions))\n",
    "\n",
    "    np.savez_compressed(f'C:/Users/oem/Desktop/jhy/array/{start_time}/{group_idx}_XData',x_data)\n",
    "    np.savez_compressed(f'C:/Users/oem/Desktop/jhy/array/{start_time}/{group_idx}_YData',y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645 개의 액션이 저장되어있습니다.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 512)               1370112   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1645)              843885    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,213,997\n",
      "Trainable params: 2,213,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 저장파일 불러오기\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('C:/Users/oem/Desktop/jhy/signlanguage/Sign_Language_Remaster/logs/act_list.pkl', 'rb') as file:\n",
    "    # 리스트 로드\n",
    "    actions = pickle.load(file)\n",
    "    print(len(actions),'개의 액션이 저장되어있습니다.')\n",
    "\n",
    "#model 1\n",
    "\n",
    "#모델 생성\n",
    "model = Sequential([\n",
    "    LSTM(512, activation='tanh', input_shape=(30,156)),\n",
    "    # LSTM(512, activation='tanh', input_shape=x_train.shape[1:3]),\n",
    "    Dense(len(actions), activation='softmax') #### @주의 모델별로 actions내용을 다르게 잡아주는게 좋을지도..?\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m x_data, y_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# batch_size=BATCH_SIZE,\u001b[39;49;00m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[\u001b[39;49;00m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\u001b[39;49;00m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ]\u001b[39;49;00m\n\u001b[0;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# VERSION = 43\u001b[39;00m\n\u001b[0;32m     56\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_test\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_G\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(actions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mact_e\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_C\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_CUT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_B\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\oem\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\oem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "VERSION = 100\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 0\n",
    "SUB_PATH = '2024-02-24_21-13-55'\n",
    "# 모델이 저장될 폴더 생성\n",
    "folder_path = f'C:/Users/oem/Desktop/jhy/signlanguage/Sign_Language_Remaster/model/{SUB_PATH}'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# 하위 파일 수 확인\n",
    "items = os.listdir(f'C:/Users/oem/Desktop/jhy/array/{SUB_PATH}')\n",
    "file_count = sum(1 for item in items if os.path.isfile(os.path.join(f'C:/Users/oem/Desktop/jhy/array/{SUB_PATH}', item)))\n",
    "print(int(file_count/2))\n",
    "\n",
    "for i in range(int(file_count/2)):\n",
    "    x_data = np.load(f'C:/Users/oem/Desktop/jhy/array/{SUB_PATH}/{i}_XData.npz')\n",
    "    y_data = np.load(f'C:/Users/oem/Desktop/jhy/array/{SUB_PATH}/{i}_YData.npz')\n",
    "    x_data = x_data['arr_0']\n",
    "    y_data = y_data['arr_0']\n",
    "    # data를 train과 test로 나눔\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "\n",
    "    DATA_CUT = 0\n",
    "    # 데이터 자르기\n",
    "    if DATA_CUT:\n",
    "        x_train = x_train[::DATA_CUT]\n",
    "        y_train = y_train[::DATA_CUT]\n",
    "        # x_test = x_test[::DATA_CUT]\n",
    "        # y_test = y_test[::DATA_CUT]\n",
    "    # x_train, x_test, y_train, y_test의 크기 확인\n",
    "    # print(f\"x_train shape: {x_train.shape}\")\n",
    "    # print(f\"y_train shape: {y_train.shape}\")\n",
    "    # print(f\"x_test shape: {x_test.shape}\")\n",
    "    # print(f\"y_test shape: {y_test.shape}\")\n",
    "    # import sys\n",
    "\n",
    "    # print('x_train size :',sys.getsizeof(x_train))\n",
    "    # print('y_train size :',sys.getsizeof(y_train))\n",
    "\n",
    "    # 메모리 정리\n",
    "    x_data, y_data = 0,0\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        # batch_size=BATCH_SIZE,\n",
    "        shuffle = False,\n",
    "        # callbacks=[\n",
    "        #     ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        #     ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "        # ]\n",
    "    )\n",
    "    # VERSION = 43\n",
    "    MODEL_NAME = f'lstm_test{VERSION}_G{i}_{len(actions)}act_e{EPOCHS}_C{DATA_CUT}_B{BATCH_SIZE}'\n",
    "    model.save(f'C:/Users/oem/Desktop/jhy/signlanguage/Sign_Language_Remaster/model/{SUB_PATH}/{MODEL_NAME}.h5')\n",
    "    print(f'model{i} saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper right')\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "plt.text(0.5, 0.5, f\"Graph generated on {current_time}\\n{MODEL_NAME}\", horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.savefig(f'{MODEL_NAME}.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
