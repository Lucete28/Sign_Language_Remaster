{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import getsizeof\n",
    "from pympler import asizeof\n",
    "import pandas as pd\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "# 카메라 캡처를 초기화합니다.\n",
    "cap = cv2.VideoCapture(0)  # 0은 기본 카메라를 의미합니다.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "# 녹화 상태를 나타내는 변수를 초기화합니다.\n",
    "recording = False\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # 비디오 코덱을 설정합니다.\n",
    "out = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    with mp_hands.Hands(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "        # 카메라에서 프레임을 읽어옵니다.\n",
    "        ret, frame = cap.read()\n",
    "        success, image = cap.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "    #     # 프레임을 화면에 표시합니다.\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # 's' 키를 누르면 녹화를 시작합니다.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "            recording = True\n",
    "            out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame.shape[1], frame.shape[0]))  # 출력 파일을 설정합니다.\n",
    "\n",
    "        # 녹화 중일 때 프레임을 저장합니다.\n",
    "        if recording:\n",
    "            out.write(frame)\n",
    "\n",
    "        # 't' 키를 누르면 녹화를 중지하고 저장 파일을 닫습니다.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('t'):\n",
    "            if recording:\n",
    "                recording = False\n",
    "                out.release()  # 녹화 파일을 닫습니다.\n",
    "                print(\"녹화가 종료되었습니다.\")\n",
    "        \n",
    "        # 'q' 키를 누르면 루프를 종료합니다.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# 카메라 캡처를 해제하고 창을 닫습니다.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = None\n",
    "recording = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 화면에 손 감지된 랜드마크 표시\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # 녹화 시작\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        recording = True\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # 녹화 중지 및 파일 저장\n",
    "    if cv2.waitKey(1) & 0xFF == ord('t'):\n",
    "        if recording:\n",
    "            recording = False\n",
    "            out.release()\n",
    "            print(\"녹화가 종료되었습니다.\")\n",
    "\n",
    "    # 녹화 중일 때 프레임 저장\n",
    "    if recording:\n",
    "        out.write(frame)\n",
    "\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = None\n",
    "recording = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 손 감지 및 랜드마크 추출\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크만 추출하여 이진 이미지 생성 (손의 랜드마크만 흰색으로, 배경은 검은색으로)\n",
    "            hand_landmarks_image = np.zeros_like(frame)\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "                cv2.circle(hand_landmarks_image, (x, y), 5, (255, 255, 255), -1)\n",
    "\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # 녹화 시작\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        recording = True\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # 녹화 중지 및 파일 저장\n",
    "    if cv2.waitKey(1) & 0xFF == ord('t'):\n",
    "        if recording:\n",
    "            recording = False\n",
    "            out.release()\n",
    "            print(\"녹화가 종료되었습니다.\")\n",
    "\n",
    "    # 녹화 중일 때 프레임 저장\n",
    "    if recording:\n",
    "        out.write(hand_landmarks_image)\n",
    "\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안녕하세요, 점심, 빵, 먹다, 가다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = None\n",
    "recording = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 손 감지 및 랜드마크 추출\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크만 추출하여 이진 이미지 생성 (손의 랜드마크만 흰색으로, 배경은 검은색으로)\n",
    "            hand_landmarks_image = np.zeros_like(frame)\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                x, y = int(lm.x * frame.shape[1]), int(lm.y * frame.shape[0])\n",
    "                cv2.circle(hand_landmarks_image, (x, y), 5, (255, 255, 255), -1)\n",
    "\n",
    "            # 랜드마크 간의 선 그리기\n",
    "            connections = mp_hands.HAND_CONNECTIONS\n",
    "            for connection in connections:\n",
    "                connection_point_a = tuple(\n",
    "                    np.multiply([hand_landmarks.landmark[connection[0]].x, hand_landmarks.landmark[connection[0]].y],\n",
    "                                [frame.shape[1], frame.shape[0]]).astype(int))\n",
    "                connection_point_b = tuple(\n",
    "                    np.multiply([hand_landmarks.landmark[connection[1]].x, hand_landmarks.landmark[connection[1]].y],\n",
    "                                [frame.shape[1], frame.shape[0]]).astype(int))\n",
    "                cv2.line(hand_landmarks_image, connection_point_a, connection_point_b, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Video', hand_landmarks_image)\n",
    "\n",
    "    # 녹화 시작\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        recording = True\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # 녹화 중지 및 파일 저장\n",
    "    if cv2.waitKey(1) & 0xFF == ord('t'):\n",
    "        if recording:\n",
    "            recording = False\n",
    "            out.release()\n",
    "            print(\"녹화가 종료되었습니다.\")\n",
    "\n",
    "    # 녹화 중일 때 프레임 저장\n",
    "    if recording:\n",
    "        out.write(hand_landmarks_image)\n",
    "\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "녹화가 종료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = None\n",
    "recording = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 손 감지 및 랜드마크 추출\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 검은색 배경 생성\n",
    "    hand_landmarks_image = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크를 화면에 표시\n",
    "            mp_drawing.draw_landmarks(hand_landmarks_image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "                                      landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n",
    "                                      connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2))\n",
    "\n",
    "    cv2.imshow('Video', hand_landmarks_image)\n",
    "\n",
    "    # 녹화 시작\n",
    "    if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "        recording = True\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    # 녹화 중지 및 파일 저장\n",
    "    if cv2.waitKey(1) & 0xFF == ord('t'):\n",
    "        if recording:\n",
    "            recording = False\n",
    "            out.release()\n",
    "            print(\"녹화가 종료되었습니다.\")\n",
    "\n",
    "    # 녹화 중일 때 프레임 저장\n",
    "    if recording:\n",
    "        out.write(hand_landmarks_image)\n",
    "\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프레임 수: 122\n",
      "높이: 1080\n",
      "너비: 1920\n",
      "채널 수: 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# AVI 파일 경로 설정\n",
    "avi_file_path = r'C:/PlayData/sign_remaster/Sign_Language_Remaster/data/hello/hello_9.avi'\n",
    "\n",
    "# AVI 파일 열기\n",
    "video_capture = cv2.VideoCapture(avi_file_path)\n",
    "\n",
    "# 영상의 프레임 수, 높이, 너비, 채널 수 확인\n",
    "frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "channels = 3  # AVI 파일의 경우 RGB 채널이므로 3\n",
    "\n",
    "print(\"프레임 수:\", frame_count)\n",
    "print(\"높이:\", frame_height)\n",
    "print(\"너비:\", frame_width)\n",
    "print(\"채널 수:\", channels)\n",
    "\n",
    "# AVI 파일 닫기\n",
    "video_capture.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\PlayData\\\\sign_remaster\\\\Sign_Language_Remaster\\\\data\\\\bread_house\\\\', 'C:\\\\PlayData\\\\sign_remaster\\\\Sign_Language_Remaster\\\\data\\\\hello\\\\']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 30, 128, 128, 3, 1)\n",
      "(4, 30, 128, 128, 3, 1)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_11' (type Sequential).\n    \n    Input 0 of layer \"conv3d_21\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (16, 30, 128, 128, 3)\n    \n    Call arguments received by layer 'sequential_11' (type Sequential):\n      • inputs=tf.Tensor(shape=(16, 30, 128, 128, 3, 1), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\PlayData\\sign_remaster\\Sign_Language_Remaster\\src\\first.ipynb 셀 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X11sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X11sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m# 모델 훈련\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X11sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_data_tf, train_labels_tf, epochs\u001b[39m=\u001b[39;49mnum_epochs, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X11sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m \u001b[39m# 테스트 데이터를 텐서로 변환\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X11sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m test_data_tf \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(test_data, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filef6wwecjk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 280, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_11' (type Sequential).\n    \n    Input 0 of layer \"conv3d_21\" is incompatible with the layer: expected axis -1 of input shape to have value 1, but received input with shape (16, 30, 128, 128, 3)\n    \n    Call arguments received by layer 'sequential_11' (type Sequential):\n      • inputs=tf.Tensor(shape=(16, 30, 128, 128, 3, 1), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 폴더 경로\n",
    "data_folder = r'C:\\PlayData\\sign_remaster\\Sign_Language_Remaster\\data'\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def adjust_frames(video_frames, target_frame_count, target_height, target_width):\n",
    "    total_frames = len(video_frames)\n",
    "    if total_frames >= target_frame_count:\n",
    "        # 동영상의 프레임 수가 목표 프레임 수보다 크거나 같으면 샘플링하여 조정\n",
    "        step = total_frames // target_frame_count\n",
    "        adjusted_frames = video_frames[::step][:target_frame_count]\n",
    "    else:\n",
    "        # 동영상의 프레임 수가 목표 프레임 수보다 작으면 선형 보간하여 조정\n",
    "        interpolation_factor = total_frames / target_frame_count\n",
    "        indices = np.arange(0, total_frames, interpolation_factor)[:target_frame_count]\n",
    "        adjusted_frames = np.array([cv2.resize(frame, (target_width, target_height)) for frame in video_frames])[indices.astype(int)]\n",
    "    return np.array(adjusted_frames)\n",
    "\n",
    "\n",
    "# load_and_preprocess_data 함수 수정\n",
    "def load_and_preprocess_data(file_paths, labels, target_frame_count, target_height, target_width):\n",
    "    frames = []\n",
    "    for file_path in file_paths:\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        frame_list = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (target_width, target_height))  # 모든 프레임을 동일한 크기로 조정\n",
    "            frame_list.append(frame)\n",
    "        cap.release()\n",
    "        # 프레임 수 조정\n",
    "        adjusted_frames = adjust_frames(frame_list, target_frame_count,128,128)\n",
    "        frames.append(adjusted_frames)\n",
    "    data = np.array(frames)\n",
    "    labels = np.array(labels)\n",
    "    data = data / 255.0\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    labels_encoded = labels_encoded.reshape(len(labels_encoded), 1)\n",
    "    labels_onehot = onehot_encoder.fit_transform(labels_encoded)\n",
    "    return data, labels_onehot\n",
    "\n",
    "target_frame_count = 30  # 목표 프레임 수 (예: 30프레임/초 동영상일 경우 1초)\n",
    "target_height = 128\n",
    "target_width = 128\n",
    "\n",
    "# 데이터 불러오고 나누는 함수\n",
    "def load_and_split_data(data_folder, test_size=0.2, random_state=42):\n",
    "    # data 폴더 아래의 모든 라벨 폴더를 찾기\n",
    "    label_folders = glob.glob(os.path.join(data_folder, '*\\\\'))\n",
    "    \n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    print(label_folders)\n",
    "    # 각 라벨 폴더에서 파일 경로와 레이블 수집\n",
    "    for label_folder in label_folders:\n",
    "        label = os.path.basename(os.path.normpath(label_folder))  # 라벨은 폴더 이름에서 추출\n",
    "        files = glob.glob(os.path.join(label_folder, '*.avi'))\n",
    "        file_paths.extend(files)\n",
    "        labels.extend([label] * len(files))\n",
    "\n",
    "    # 데이터를 무작위로 섞고 train-test split 수행\n",
    "    train_file_paths, test_file_paths, train_labels, test_labels = train_test_split(\n",
    "        file_paths, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    # 데이터 전처리 (load_and_preprocess_data 함수 사용)\n",
    "    train_data, train_labels = load_and_preprocess_data(train_file_paths, train_labels, target_frame_count=30, target_width=128,target_height=128)\n",
    "    test_data, test_labels = load_and_preprocess_data(test_file_paths, test_labels,target_frame_count=30, target_width=128,target_height=128)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "# 3D CNN 모델 구축 함수\n",
    "def build_3d_cnn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_model(model, train_data, train_labels, num_epochs, batch_size):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, train_labels, epochs=num_epochs, batch_size=batch_size)\n",
    "    return history\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, test_data, test_labels):\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "    return loss, accuracy\n",
    "\n",
    "# 데이터 불러오고 나누기\n",
    "train_data, train_labels, test_data, test_labels = load_and_split_data(data_folder)\n",
    "\n",
    "# 데이터 형태 조정 (채널 추가)\n",
    "train_data = np.expand_dims(train_data, axis=-1)\n",
    "test_data = np.expand_dims(test_data, axis=-1)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "train_data_tf = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "train_labels_tf = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "\n",
    "# 모델 구축\n",
    "# input_shape = train_data.shape[1:]  # (frames, height, width, channels)\n",
    "input_shape = (target_frame_count, target_height, target_width, 1)\n",
    "num_classes = len(np.unique(train_labels))\n",
    "# model = build_3d_cnn_model(input_shape, num_classes)\n",
    "model = build_3d_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(train_data_tf, train_labels_tf, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# 테스트 데이터를 텐서로 변환\n",
    "test_data_tf = tf.convert_to_tensor(test_data, dtype=tf.float32)\n",
    "test_labels_tf = tf.convert_to_tensor(test_labels, dtype=tf.float32)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_accuracy = model.evaluate(test_data_tf, test_labels_tf)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data count: 16\n",
      "Test data count: 4\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(folder_path, image_size=(128, 128), test_size=0.2, random_seed=42):\n",
    "    labels = []\n",
    "    data = []\n",
    "\n",
    "    for label, subfolder in enumerate(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith(\".avi\"):\n",
    "                    video_path = os.path.join(subfolder_path, filename)\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    frames = []\n",
    "\n",
    "                    while cap.isOpened():\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        resized_frame = cv2.resize(frame, image_size)\n",
    "                        frames.append(resized_frame)\n",
    "\n",
    "                    frames = np.array(frames)\n",
    "                    # 각 프레임을 여기서 처리하거나 사용하십시오.\n",
    "\n",
    "                    labels.append(label)\n",
    "                    data.append(frames)\n",
    "\n",
    "                    cap.release()\n",
    "\n",
    "    # 라벨링 및 데이터 수집 완료 후, train/test 데이터 분할 코드 추가\n",
    "    data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=test_size, random_state=random_seed)\n",
    "    # print(\"Train data shape:\", train_data.shape)\n",
    "    # print(\"Train labels shape:\", train_labels.shape)\n",
    "    # print(\"Test data shape:\", test_data.shape)\n",
    "    # print(\"Test labels shape:\", test_labels.shape)\n",
    "    # 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "    print(\"Train data count:\", len(data_train))\n",
    "    print(\"Test data count:\", len(data_test))\n",
    "\n",
    "    return data_train, data_test, labels_train, labels_test\n",
    "\n",
    "# 사용 예시\n",
    "folder_path = r\"C:\\PlayData\\sign_remaster\\Sign_Language_Remaster\\data\"  # AVI 파일이 있는 폴더 경로 입력\n",
    "train_data, test_data, train_labels, test_labels = load_and_preprocess_data(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (16, 90, 128, 128, 3)\n",
      "Train labels shape: (16, 2)\n",
      "Test data shape: (4, 90, 128, 128, 3)\n",
      "Test labels shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_preprocess_data(folder_path, target_frame_count=90, image_size=(128, 128), test_size=0.2, random_seed=42):\n",
    "    labels = []\n",
    "    data = []\n",
    "\n",
    "    for label, subfolder in enumerate(os.listdir(folder_path)):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith(\".avi\"):\n",
    "                    video_path = os.path.join(subfolder_path, filename)\n",
    "                    cap = cv2.VideoCapture(video_path)\n",
    "                    frames = []\n",
    "\n",
    "                    while cap.isOpened() and len(frames) < target_frame_count:\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            break\n",
    "                        resized_frame = cv2.resize(frame, image_size)\n",
    "                        frames.append(resized_frame)\n",
    "\n",
    "                    # 프레임 수가 부족하면 패딩을 추가\n",
    "                    while len(frames) < target_frame_count:\n",
    "                        frames.append(np.zeros_like(frames[0]))\n",
    "\n",
    "                    frames = np.array(frames)\n",
    "                    # 여기서 frames를 사용하여 모델에 데이터로 사용할 수 있음\n",
    "\n",
    "                    labels.append(label)\n",
    "                    data.append(frames)\n",
    "\n",
    "                    cap.release()\n",
    "\n",
    "    # 데이터를 4D 텐서로 변환 (샘플 수, 프레임 수, 높이, 너비)\n",
    "    data = np.array(data)\n",
    "\n",
    "    # 데이터를 0~1 사이의 값으로 정규화\n",
    "    data = data.astype('float32') / 255.0\n",
    "\n",
    "    # 라벨을 원-핫 인코딩으로 변환\n",
    "    labels = np.array(labels)\n",
    "    num_classes = len(os.listdir(folder_path))\n",
    "    labels = np.eye(num_classes)[labels]\n",
    "\n",
    "    # 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "    print(\"Train data shape:\", train_data.shape)\n",
    "    print(\"Train labels shape:\", train_labels.shape)\n",
    "    print(\"Test data shape:\", test_data.shape)\n",
    "    print(\"Test labels shape:\", test_labels.shape)\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "# 사용 예시\n",
    "folder_path = r\"C:\\PlayData\\sign_remaster\\Sign_Language_Remaster\\data\"  # AVI 파일이 있는 폴더 경로 입력\n",
    "train_data, test_data, train_labels, test_labels = load_and_preprocess_data(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n",
      "(90, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data)):\n",
    "    print(train_data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 전처리된 파일 재생\n",
    "import cv2\n",
    "\n",
    "# 비디오 재생을 위한 함수 정의\n",
    "def play_video(frames):\n",
    "    for frame in frames:\n",
    "        cv2.imshow(\"Video\", frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):  # 'q' 키를 누르면 종료\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# train_data 중 첫 번째 비디오 재생\n",
    "for i in range(len(train_data)):\n",
    "    play_video(train_data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 50s 16s/step - loss: 0.6413 - accuracy: 0.7500 - val_loss: 0.5748 - val_accuracy: 0.7500\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 49s 17s/step - loss: 0.1891 - accuracy: 1.0000 - val_loss: 1.0867 - val_accuracy: 0.7500\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 49s 17s/step - loss: 8.1812e-04 - accuracy: 1.0000 - val_loss: 2.4249 - val_accuracy: 0.7500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.8477e-06 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "def create_3d_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # 3D Convolution layers\n",
    "    model.add(layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "    model.add(layers.Conv3D(64, (3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "    model.add(layers.Conv3D(64, (3, 3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling3D((2, 2, 2)))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "# 모델 생성\n",
    "input_shape = train_data.shape[1:]\n",
    "num_classes = len(os.listdir(folder_path))\n",
    "model = create_3d_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(train_data, train_labels, epochs=3, batch_size=4, validation_split=0.2)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:/PlayData/sign_remaster/Sign_Language_Remaster/model/test10.h5\n"
     ]
    }
   ],
   "source": [
    "# 모델을 저장할 경로 \n",
    "model_save_path = 'C:/PlayData/sign_remaster/Sign_Language_Remaster/model/test10.h5'\n",
    "\n",
    "# 모델 저장\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "num = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = None\n",
    "recording = False\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 손 감지 및 랜드마크 추출\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 검은색 배경 생성\n",
    "    hand_landmarks_image = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크를 화면에 표시\n",
    "            mp_drawing.draw_landmarks(hand_landmarks_image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "                                      landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n",
    "                                      connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2))\n",
    "\n",
    "    cv2.imshow('Video', hand_landmarks_image)\n",
    "\n",
    "    # 여기서 이미지 전처리(입력크기는 90,128,128,3)\n",
    "    # 모델에 90프레임을 입력하도록 처리\n",
    "    pred = model.predict(hand_landmarks_image)\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/conv3d_2/Conv3D' defined at (most recent call last):\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\2580j\\AppData\\Local\\Temp\\ipykernel_16348\\1224703346.py\", line 55, in <module>\n      pred = model.predict(input_video)  # 모델에 예측 요청\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'sequential/conv3d_2/Conv3D'\nComputed output size would be negative: -2 [input_size: 0, effective_filter_size: 3, stride: 1]\n\t [[{{node sequential/conv3d_2/Conv3D}}]] [Op:__inference_predict_function_2757]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\PlayData\\sign_remaster\\Sign_Language_Remaster\\src\\first.ipynb 셀 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X24sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m input_video \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(input_video, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# 배치 차원 추가\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X24sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# 모델에 예측 요청\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X24sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(input_video)  \u001b[39m# 모델에 예측 요청\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# 여기서 예측값(pred)를 사용할 수 있습니다.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/PlayData/sign_remaster/Sign_Language_Remaster/src/first.ipynb#X24sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(pred)\n",
      "File \u001b[1;32mc:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/conv3d_2/Conv3D' defined at (most recent call last):\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\2580j\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\2580j\\AppData\\Local\\Temp\\ipykernel_16348\\1224703346.py\", line 55, in <module>\n      pred = model.predict(input_video)  # 모델에 예측 요청\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"c:\\Users\\2580j\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'sequential/conv3d_2/Conv3D'\nComputed output size would be negative: -2 [input_size: 0, effective_filter_size: 3, stride: 1]\n\t [[{{node sequential/conv3d_2/Conv3D}}]] [Op:__inference_predict_function_2757]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 모델 로드\n",
    "# model = load_model(\"your_model_path.h5\")  # 모델 파일 경로를 입력하세요\n",
    "\n",
    "num = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# 9개의 빈 배열 초기화\n",
    "video_frames = [np.zeros((128, 128, 3), dtype=np.float32) for _ in range(9)]\n",
    "frame_count = 0  # 현재까지 저장된 프레임 수\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 손 감지 및 랜드마크 추출\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 검은색 배경 생성\n",
    "    hand_landmarks_image = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크를 화면에 표시\n",
    "            mp_drawing.draw_landmarks(hand_landmarks_image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "                                      landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n",
    "                                      connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2))\n",
    "\n",
    "    # 이미지를 전처리하고 비디오 프레임 배열에 추가\n",
    "    resized_image = cv2.resize(hand_landmarks_image, (128, 128))  # 입력 크기에 맞게 조정\n",
    "    resized_image = resized_image / 255.0  # 정규화\n",
    "\n",
    "    # 현재 프레임을 적절한 배열에 추가\n",
    "    if frame_count % 10 == 0 and frame_count > 0:\n",
    "        target_index = (frame_count // 10 - 1) % 9\n",
    "        video_frames[target_index] = resized_image\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # 90프레임이 모였을 때 모델에 전달하여 예측\n",
    "    if frame_count >= 90:\n",
    "        input_video = np.array(video_frames)  # 90프레임을 4D 배열로 변환\n",
    "        input_video = np.expand_dims(input_video, axis=0)  # 배치 차원 추가\n",
    "\n",
    "        # 모델에 예측 요청\n",
    "        pred = model.predict(input_video)  # 모델에 예측 요청\n",
    "        # 여기서 예측값(pred)를 사용할 수 있습니다.\n",
    "        print(pred)\n",
    "\n",
    "        # 프레임 카운트 및 비디오 프레임 배열 초기화\n",
    "        frame_count = 0\n",
    "        video_frames = [np.zeros((128, 128, 3), dtype=np.float32) for _ in range(9)]\n",
    "\n",
    "    cv2.imshow('Video', hand_landmarks_image)\n",
    "\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 88, 126, 126, 32)  2624      \n",
      "                                                                 \n",
      " max_pooling3d (MaxPooling3D  (None, 44, 63, 63, 32)   0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 42, 61, 61, 64)    55360     \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 21, 30, 30, 64)   0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_2 (Conv3D)           (None, 19, 28, 28, 64)    110656    \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 9, 14, 14, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 112896)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                7225408   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,394,178\n",
      "Trainable params: 7,394,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 294ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 297ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 322ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 311ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 268ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 453ms/step\n",
      "Predicted Label: bread_house\n",
      "1/1 [==============================] - 0s 261ms/step\n",
      "Predicted Label: bread_house\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 모델 로드\n",
    "# model = load_model(\"your_model_path.h5\") \n",
    "\n",
    "num = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# 88프레임을 담을 빈 배열 초기화\n",
    "video_frames = [np.zeros((128, 128, 3), dtype=np.float32) for _ in range(88)]\n",
    "frame_count = 0  # 현재까지 저장된 프레임 수\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    # 손 감지 및 랜드마크 추출\n",
    "    results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 검은색 배경 생성\n",
    "    hand_landmarks_image = np.zeros_like(frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # 손 랜드마크를 화면에 표시\n",
    "            mp_drawing.draw_landmarks(hand_landmarks_image, hand_landmarks, mp_hands.HAND_CONNECTIONS, \n",
    "                                      landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2),\n",
    "                                      connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=2))\n",
    "\n",
    "    # 이미지를 전처리하고 비디오 프레임 배열에 추가\n",
    "    resized_image = cv2.resize(hand_landmarks_image, (128, 128))  # 입력 크기에 맞게 조정\n",
    "    resized_image = resized_image / 255.0  # 정규화\n",
    "\n",
    "    # 현재 프레임을 빈 배열 중 적절한 위치에 추가\n",
    "    target_index = frame_count % 88\n",
    "    video_frames[target_index] = resized_image\n",
    "    frame_count += 1\n",
    "\n",
    "    # 88프레임이 모였을 때 모델에 전달하여 예측\n",
    "    if frame_count >= 88:\n",
    "        input_video = np.array(video_frames)  # 88프레임을 4D 배열로 변환\n",
    "        input_video = np.expand_dims(input_video, axis=0)  # 배치 차원 추가\n",
    "\n",
    "        # 모델에 예측 요청\n",
    "        pred = model.predict(input_video)  # 모델에 예측 요청\n",
    "\n",
    "        # 예측된 클래스를 확인하고 라벨 출력\n",
    "        predicted_class = np.argmax(pred, axis=1)  # 확률이 가장 높은 클래스의 인덱스 가져오기\n",
    "        if predicted_class == 0:\n",
    "            print(\"Predicted Label: house\")\n",
    "        else:\n",
    "            print(\"Predicted Label: bread_house\")\n",
    "\n",
    "    cv2.imshow('Video', hand_landmarks_image)\n",
    "\n",
    "    # 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_frames = [np.zeros((128, 128, 3), dtype=np.float32) for _ in range(88)]\n",
    "video_frames[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
