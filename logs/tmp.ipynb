{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# 대상 폴더 경로 설정\n",
    "target_folder = r'C:\\Users\\oem\\Desktop\\jhy\\dataset'\n",
    "\n",
    "# 대상 폴더 내의 모든 하위 폴더에 대해 반복\n",
    "for root, dirs, _ in os.walk(target_folder):\n",
    "    for dir in dirs:\n",
    "        # 현재 하위 폴더 내의 'seq'로 시작하는 모든 파일 찾기\n",
    "        seq_files = glob.glob(os.path.join(root, dir, 'seq*'))\n",
    "        print(os.path.join(root, dir, 'seq*'))\n",
    "        print(seq_files)\n",
    "        # 'seq'로 시작하는 파일이 정확히 하나만 있는 경우\n",
    "        if len(seq_files) == 1:\n",
    "            # 파일 삭제\n",
    "            os.remove(seq_files[0])\n",
    "            # print(f\"'{seq_files[0]}' 파일이 삭제되었습니다.\")\n",
    "        else:\n",
    "            print(f'{os.path.join(root, dir)} 폴더에 파일이 2개이상잇습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def write_txt_log(T_path, text):\n",
    "    current_time = datetime.now()\n",
    "    formatted_time = current_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    with open(T_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(f\"{formatted_time} ::: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 열기 및 읽기\n",
    "with open(r'C:\\Users\\oem\\Desktop\\jhy\\signlanguage\\Sign_Language_Remaster\\logs\\api_log.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 확인된 단어를 추적하는 딕셔너리\n",
    "seen_words = {}\n",
    "count=0\n",
    "# 모든 페이지 및 단어 순회\n",
    "for page_num, words in data['Daily'].items():\n",
    "    for word in list(words):  # list()를 사용하여 순회 중 딕셔너리 변경 문제 방지\n",
    "        if word in seen_words:\n",
    "            # 단어가 이미 존재하면, 키 변경\n",
    "            new_key = f\"{word}_in{page_num}\"\n",
    "            data['Daily'][page_num][new_key] = data['Daily'][page_num].pop(word)\n",
    "            print(word,'-->',new_key)\n",
    "            count+=1\n",
    "            write_txt_log(r'C:\\Users\\oem\\Desktop\\jhy\\signlanguage\\Sign_Language_Remaster\\logs\\LOG.TXT',f'{word} in page{page_num} changed!')\n",
    "        else:\n",
    "            # 단어가 처음 나타나면, 추적 딕셔너리에 추가\n",
    "            seen_words[word] = True\n",
    "\n",
    "# # 변경된 딕셔너리를 다시 JSON 파일로 저장\n",
    "with open(r'C:\\Users\\oem\\Desktop\\jhy\\signlanguage\\Sign_Language_Remaster\\logs\\api_log.json', 'w',encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\abstinence\n",
      "abstinence\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Association\n",
      "Association\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\bald head\n",
      "bald head\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Ceremony\n",
      "Ceremony\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\clarify\n",
      "clarify\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\confrontation\n",
      "confrontation\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Counter\n",
      "Counter\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\dog\n",
      "dog\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\frame\n",
      "frame\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\fruit\n",
      "fruit\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Get\n",
      "Get\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\head\n",
      "head\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\History\n",
      "History\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\hold\n",
      "hold\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\individual\n",
      "individual\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\like\n",
      "like\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\loyalty\n",
      "loyalty\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\mental\n",
      "mental\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Meter\n",
      "Meter\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Octopus\n",
      "Octopus\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\outside\n",
      "outside\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Pass\n",
      "Pass\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Personality\n",
      "Personality\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Plaster\n",
      "Plaster\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Regret\n",
      "Regret\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\report\n",
      "report\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Rose\n",
      "Rose\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\run\n",
      "run\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\school\n",
      "school\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\seizure\n",
      "seizure\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\skip\n",
      "skip\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Sleep\n",
      "Sleep\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\suddenly\n",
      "suddenly\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Take\n",
      "Take\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\the opposite\n",
      "the opposite\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Train station\n",
      "Train station\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\training\n",
      "training\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Twist\n",
      "Twist\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\University\n",
      "University\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\Volunteer\n",
      "Volunteer\n",
      "C:\\Users\\oem\\Desktop\\jhy\\dataset\\win\n",
      "win\n",
      "41 개의 폴더 발견\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def find_folders_with_files(root_path, min_file_count=3):\n",
    "    count = 0\n",
    "    e_li = []\n",
    "    # root_path 아래의 모든 폴더를 탐색\n",
    "    for folder_name, subfolders, filenames in os.walk(root_path):\n",
    "        # 현재 폴더에 있는 파일 수가 min_file_count 이상인 경우 폴더 이름 출력\n",
    "        if len(filenames) >= min_file_count:\n",
    "            print(folder_name)\n",
    "            count +=1\n",
    "            e_li.append(folder_name.split('\\\\')[-1])\n",
    "            \n",
    "    print(count,'개의 폴더 발견')\n",
    "\n",
    "# 함수를 호출하여 특정 경로 아래 폴더 탐색 시작\n",
    "# 예를 들어, 'C:/example_path'를 원하는 경로로 바꿔주세요.\n",
    "find_folders_with_files(r'C:\\Users\\oem\\Desktop\\jhy\\dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_list = ['Rule','Public','Defender','shed','Job','Poverty','theory','hot','me','(Shooting gun)','impression','Japanese','victim','please','suddenly','head','shoulder','owner','fly','Defeat','pass','hair','seizure','outside','every','Training','Tear','suddenly','sad','temperature','cold','joke','mask','Great','three','army','artist','answer','Anniversary','(Time) Poetry','divide','poor','Northern','stack','Be','outside','snack','Penalty','Declaration','aunt','pink','dry','young','author','Independence','Queen','pizza','deputy','Highly','Sip','length','go','explanation','tie','take','Toilet','repair','green','subject','income','stop','never','method','join','sister','yes','you','new year','Get','date','Work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = ['_in12','_in27','_in33','_in35','_in39','_in44','_in46','_in57','_in60','_in61','_in64','_in64','_in70','_in77','_in79','_in81','_in85','_in87','_in88','_in91','_in93','_in94','_in95','_in97','_in98','_in99','_in100','_in100','_in108','_in111','_in112','_in112','_in113','_in116','_in118','_in118','_in119','_in120','_in123','_in124','_in124','_in125','_in126','_in126','_in127','_in129','_in130','_in131','_in133','_in133','_in135','_in135','_in137','_in137','_in137','_in137','_in138','_in140','_in140','_in141','_in142','_in144','_in145','_in147','_in147','_in150','_in151','_in155','_in156','_in156','_in157','_in159','_in160','_in160','_in161','_in161','_in162','_in165','_in166','_in167','_in169']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder_names = []\n",
    "for i in range(len(a)):\n",
    "    new_folder_names.append(f'{english_word_list[i]}{a[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_word_list.reverse()\n",
    "new_folder_names.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_word_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(target_path, folder_name)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 폴더가 영어 단어 리스트에 포함되어 있는지 확인\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m folder_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43menglish_word_list\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder_path):\n\u001b[0;32m     14\u001b[0m     most_recent_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     most_recent_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'english_word_list' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 대상 경로, 영어 단어 리스트와 변경될 폴더 이름 리스트\n",
    "target_path = r'C:\\Users\\oem\\Desktop\\jhy\\dataset'\n",
    "\n",
    "\n",
    "# 지정된 경로 아래의 모든 폴더를 탐색\n",
    "for folder_name in os.listdir(target_path):\n",
    "    folder_path = os.path.join(target_path, folder_name)\n",
    "\n",
    "    # 폴더가 영어 단어 리스트에 포함되어 있는지 확인\n",
    "    if folder_name in english_word_list and os.path.isdir(folder_path):\n",
    "        most_recent_file = None\n",
    "        most_recent_time = 0\n",
    "\n",
    "        # 폴더 내의 모든 파일을 탐색하여 \"seq\"로 시작하는 가장 최근 파일 찾기\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if file_name.startswith('raw'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                creation_time = os.path.getctime(file_path)\n",
    "\n",
    "                if creation_time > most_recent_time:\n",
    "                    most_recent_file = file_path\n",
    "                    most_recent_time = creation_time\n",
    "\n",
    "        # 가장 최근에 생성된 \"seq\" 파일이 있는 경우\n",
    "        if most_recent_file:\n",
    "            # 영어 단어의 인덱스를 찾아 해당 인덱스에 해당하는 새 폴더 이름으로 이동\n",
    "            index = english_word_list.index(folder_name)\n",
    "            new_folder_path = os.path.join(target_path, new_folder_names[index])\n",
    "\n",
    "            # 새 폴더 생성 (이미 존재하는 경우 건너뜀)\n",
    "            os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "            # 파일을 새 폴더로 이동\n",
    "            shutil.move(most_recent_file, new_folder_path)\n",
    "            write_txt_log(r'C:\\Users\\oem\\Desktop\\jhy\\signlanguage\\Sign_Language_Remaster\\logs\\LOG.TXT',f\"Moved '{most_recent_file}' to '{new_folder_path}'\")\n",
    "\n",
    "            print(f\"Moved '{most_recent_file}' to '{new_folder_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_groups(data_count, group_size, appearances_per_data):\n",
    "    # 데이터셋 초기화\n",
    "    datasets = list(range(1, data_count + 1))\n",
    "    # random.shuffle(datasets)\n",
    "\n",
    "    # 각 데이터의 사용 횟수를 추적하는 딕셔너리\n",
    "    usage_count = {dataset: 0 for dataset in datasets}\n",
    "   \n",
    "    # 그룹을 저장할 리스트\n",
    "    groups = []\n",
    "   \n",
    "    # 현재 그룹\n",
    "    current_group = []\n",
    "   \n",
    "    # 데이터셋을 순환하면서 그룹 생성\n",
    "    for _ in range(appearances_per_data):\n",
    "        for dataset in datasets:\n",
    "            # 현재 그룹에 데이터 추가\n",
    "            current_group.append(dataset)\n",
    "            usage_count[dataset] += 1\n",
    "           \n",
    "            # 현재 그룹이 가득 찼거나 모든 데이터가 사용된 경우 그룹 저장 및 초기화\n",
    "            if len(current_group) == group_size or all(usage_count[dataset] == appearances_per_data for dataset in datasets):\n",
    "                groups.append(current_group)\n",
    "                current_group = []\n",
    "   \n",
    "    return groups\n",
    "\n",
    "# 4000개의 데이터셋으로 크기가 500인 그룹을 만들고, 각 데이터가 3번씩 포함되도록 그룹 생성\n",
    "groups = create_groups(4000, 400, 10)\n",
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting filenoti==0.0.5\n",
      "  Downloading filenoti-0.0.5-py3-none-any.whl.metadata (352 bytes)\n",
      "Requirement already satisfied: requests>=2.25.1 in c:\\users\\oem\\anaconda3\\lib\\site-packages (from filenoti==0.0.5) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oem\\anaconda3\\lib\\site-packages (from requests>=2.25.1->filenoti==0.0.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oem\\anaconda3\\lib\\site-packages (from requests>=2.25.1->filenoti==0.0.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oem\\anaconda3\\lib\\site-packages (from requests>=2.25.1->filenoti==0.0.5) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oem\\anaconda3\\lib\\site-packages (from requests>=2.25.1->filenoti==0.0.5) (2023.11.17)\n",
      "Downloading filenoti-0.0.5-py3-none-any.whl (1.9 kB)\n",
      "Installing collected packages: filenoti\n",
      "Successfully installed filenoti-0.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install filenoti==0.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful. Response code: 200\n"
     ]
    }
   ],
   "source": [
    "from filenoti import filenoti as fn\n",
    "fn.api_key = \"0CdjiahiBHQWc3vR9dB2vUBq1uDFXPATqH9AsOcB5Yb\" # PLEASE OBTAIN A LINE NOTI API KEY\n",
    "\n",
    "with fn.main():\n",
    "    fn.noti_print('SOME WORK SUCESS') # SEND MESSAGE DURING SCRIPT\n",
    "    print(1/0) # ERROR MESSAGE SEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
